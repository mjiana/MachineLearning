가설 증명
hypothesis = tf.sigmoid(tf.matmul(X, W)+b)

입력값         XOR 결과 라벨
------------------------------
[0, 0]             0
[0, 1]             1
[1, 0]             1
[1, 1]             0

------------------------------ 여러개의 퍼셉트론이 있는 경우
가중치 W와 절편 b는 임의의 값으로 지정하여 계산해보자.
  -> 수없는 반복학습을 통하여 최적의 가중치 W와 절편 b를 구하게 된다.

------------------------------ 이전 단계
A : 입력 X 가중치 + b = 결과값 =>sigmoid=> A 출력값
  1) [0, 0]  * [5, 5] + (-8) = -8 =>sigmoid=> 0
  2) [0, 1]  * [5, 5] + (-8) = -3 =>sigmoid=> 0
  3) [1, 0]  * [5, 5] + (-8) = -3 =>sigmoid=> 0
  4) [1, 1]  * [5, 5] + (-8) = 2 =>sigmoid=> 1

B : 입력 X 가중치 + b = 결과값 =>sigmoid=> B 출력값
  1) [0, 0]  * [-7, -7] + (3) = 3 =>sigmoid=> 1
  2) [0, 1]  * [-7, -7] + (3) = -4 =>sigmoid=> 0
  3) [1, 0]  * [-7, -7] + (3) = -4 =>sigmoid=> 0
  4) [1, 1]  * [-7, -7] + (3) = -11 =>sigmoid=> 0

------------------------------ 다음 단계
이전단계의 출력을 다음단계 입력으로 받아들인다.
C : 입력 [A 출력값, B 출력값]  X 가중치 + b = 결과값 =>sigmoid=> C 출력값
  1) [0, 1]  * [-11, -11] + (6) = -5 =>sigmoid=> 0
  2) [0, 0]  * [-11, -11] + (6) = 6 =>sigmoid=> 1
  3) [0, 0]  * [-11, -11] + (6) = 6 =>sigmoid=> 1
  4) [1, 0]  * [-11, -11] + (6) = -5 =>sigmoid=> 0

------------------------------ C의 최종 출력값은 XOR 결과와 동일하다
              XOR 결과             출력값
입력값          라벨           A      B      C
----------------------------------------------------
[0, 0]           0            0      1      0
[0, 1]           1            0      0      1
[1, 0]           1            0      0      1
[1, 1]           0            1      0      0


다층 퍼셉트론 ==> nn ==> 뉴럴 네트워크 ==> 신경망회로
  ==> 은닉층이 많아지면 딥러닝이라고 부른다.
결론 : 깊어지고 넓어진다