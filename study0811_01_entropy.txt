엔트로피(entropy) : * 통계학 용어로,
            주어진 거시적 상태에 대응하는 미시적 상태의 수의 로그로 생각할 수 있다.
          * 놀람의 정도
          * ex) 축구경기 브라질 vs 중국 => 거의 브라질이 이길것이라 생각(확률로는 99%)
            -log(0.99) = 0.00436780540
            -log(0.01) = 2 <========= 이 값이 엔트로피다.
            중국이 이기는 것이 2/0.004 = 500배 더 놀랍다.

softmax 는 결과를 확률로 표시하기 때문에 엔트로피로 놀람의 정도를 계산한다.
엔트로피 : 놀람의 평균값
    ex) 0.99 * (-log(0.99)) + 0.01 * (-log(0.01)) => 0.02
    - 예측 * tf.log(예측) => hypo * tf.log(hypo)

    Q(x)로 이길확률 치환하면
    브라질이 이길확률 = 1 : -∑Q(x) * (log(Q(x))
    브라질이 이길확률+중국이 이길 확률 : -1/∑Q(x) * (log(Q(x))

크로스 엔트로피(cross_entropy) :
    - 결과를 알고 있는 경우 : 예측과 결과를 모두 사용하면 크로스 엔트로피이다.
      - 성공, 실패 등을 알고 있다면 더욱 단순해진다.
      - 라벨 * tf.log(예측) => Y * tf.log(hypo)
    Q(x)로 이길확률 치환하고, P(x)로 이긴확률(라벨)을 치환하면
    -1/∑P(x) * (log(Q(x))

이해가 안가도 아래 함수를 사용하면 된다.
실제 사용방법 :
cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y_one_hot)
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypo),axis=1))


